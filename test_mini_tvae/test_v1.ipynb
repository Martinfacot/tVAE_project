{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test mini_tVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/mfacotti/martin/tVAE_project')\n",
    "from mini_tvae import MiniTVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      Unnamed: 0               cat1           cat2   ca  sadmdte  dschdte  \\\n",
       " 0              1               COPD            NaN  Yes    11142  11151.0   \n",
       " 1              2      MOSF w/Sepsis            NaN   No    11799  11844.0   \n",
       " 2              3  MOSF w/Malignancy  MOSF w/Sepsis  Yes    12083  12143.0   \n",
       " 3              4                ARF            NaN   No    11146  11183.0   \n",
       " 4              5      MOSF w/Sepsis            NaN   No    12035  12037.0   \n",
       " ...          ...                ...            ...  ...      ...      ...   \n",
       " 5730        5731      MOSF w/Sepsis            NaN   No    11867  11900.0   \n",
       " 5731        5732                ARF            NaN   No    12199  12241.0   \n",
       " 5732        5733                ARF            NaN   No    12087  12093.0   \n",
       " 5733        5734               COPD            NaN   No    11286  11309.0   \n",
       " 5734        5735                ARF  MOSF w/Sepsis   No    10928  10935.0   \n",
       " \n",
       "        dthdte  lstctdte death  cardiohx  ...  meta  hema  seps  trauma  ortho  \\\n",
       " 0         NaN     11382    No         0  ...    No    No    No      No     No   \n",
       " 1     11844.0     11844   Yes         1  ...    No    No   Yes      No     No   \n",
       " 2         NaN     12400    No         0  ...    No    No    No      No     No   \n",
       " 3     11183.0     11182   Yes         0  ...    No    No    No      No     No   \n",
       " 4     12037.0     12036   Yes         0  ...    No    No    No      No     No   \n",
       " ...       ...       ...   ...       ...  ...   ...   ...   ...     ...    ...   \n",
       " 5730      NaN     12074    No         1  ...    No    No   Yes      No     No   \n",
       " 5731  12629.0     12628   Yes         0  ...    No    No    No      No     No   \n",
       " 5732  12396.0     12320   Yes         1  ...    No    No    No      No     No   \n",
       " 5733  11309.0     11308   Yes         0  ...    No    No    No      No     No   \n",
       " 5734  10935.0     10934   Yes         0  ...    No    No    No      No     No   \n",
       " \n",
       "       adld3p   urin1   race      income   ptid  \n",
       " 0        0.0     NaN  white  Under $11k      5  \n",
       " 1        NaN  1437.0  white  Under $11k      7  \n",
       " 2        NaN   599.0  white    $25-$50k      9  \n",
       " 3        NaN     NaN  white    $11-$25k     10  \n",
       " 4        NaN    64.0  white  Under $11k     11  \n",
       " ...      ...     ...    ...         ...    ...  \n",
       " 5730     NaN  1285.0  white  Under $11k  10270  \n",
       " 5731     0.0  1010.0  white  Under $11k  10272  \n",
       " 5732     1.0  4638.0  white  Under $11k  10273  \n",
       " 5733     NaN     NaN  white    $11-$25k  10277  \n",
       " 5734     NaN     NaN  white    $25-$50k  10278  \n",
       " \n",
       " [5735 rows x 63 columns],\n",
       " ['cat1',\n",
       "  'cat2',\n",
       "  'ca',\n",
       "  'death',\n",
       "  'sex',\n",
       "  'dth30',\n",
       "  'swang1',\n",
       "  'dnr1',\n",
       "  'ninsclas',\n",
       "  'resp',\n",
       "  'card',\n",
       "  'neuro',\n",
       "  'gastr',\n",
       "  'renal',\n",
       "  'meta',\n",
       "  'hema',\n",
       "  'seps',\n",
       "  'trauma',\n",
       "  'ortho',\n",
       "  'race',\n",
       "  'income'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_rhc_data():\n",
    "    \"\"\"Load the RHC dataset and metadata.\"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        data = pd.read_csv('rhc.csv')\n",
    "        \n",
    "        # Load the metadata file\n",
    "        with open('metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Extract discrete columns from metadata\n",
    "        discrete_columns = []\n",
    "        for column, info in metadata.get('columns', {}).items():\n",
    "            if info.get('type') == 'categorical' or info.get('sdtype') == 'categorical':\n",
    "                discrete_columns.append(column)\n",
    "                \n",
    "        # If no discrete columns found in metadata, try to infer them\n",
    "        if not discrete_columns:\n",
    "            print(\"No discrete columns found in metadata. Inferring from data types...\")\n",
    "            for col in data.columns:\n",
    "                if data[col].dtype == 'object' or data[col].dtype == 'category':\n",
    "                    discrete_columns.append(col)\n",
    "        \n",
    "        print(f\"Loaded RHC dataset with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "        print(f\"Identified {len(discrete_columns)} discrete columns\")\n",
    "        \n",
    "        return data, discrete_columns\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure 'rhc.csv' and 'metadata.json' are in the current directory.\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "load_rhc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>ca</th>\n",
       "      <th>sadmdte</th>\n",
       "      <th>dschdte</th>\n",
       "      <th>dthdte</th>\n",
       "      <th>lstctdte</th>\n",
       "      <th>death</th>\n",
       "      <th>cardiohx</th>\n",
       "      <th>...</th>\n",
       "      <th>meta</th>\n",
       "      <th>hema</th>\n",
       "      <th>seps</th>\n",
       "      <th>trauma</th>\n",
       "      <th>ortho</th>\n",
       "      <th>adld3p</th>\n",
       "      <th>urin1</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "      <th>ptid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>COPD</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11142</td>\n",
       "      <td>11151.0</td>\n",
       "      <td>11831.5</td>\n",
       "      <td>11382</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>No</td>\n",
       "      <td>11799</td>\n",
       "      <td>11844.0</td>\n",
       "      <td>11844.0</td>\n",
       "      <td>11844</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MOSF w/Malignancy</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12083</td>\n",
       "      <td>12143.0</td>\n",
       "      <td>11831.5</td>\n",
       "      <td>12400</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>white</td>\n",
       "      <td>$25-$50k</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ARF</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>No</td>\n",
       "      <td>11146</td>\n",
       "      <td>11183.0</td>\n",
       "      <td>11183.0</td>\n",
       "      <td>11182</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1927.0</td>\n",
       "      <td>white</td>\n",
       "      <td>$11-$25k</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>MOSF w/Sepsis</td>\n",
       "      <td>No</td>\n",
       "      <td>12035</td>\n",
       "      <td>12037.0</td>\n",
       "      <td>12037.0</td>\n",
       "      <td>12036</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>white</td>\n",
       "      <td>Under $11k</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               cat1           cat2   ca  sadmdte  dschdte  \\\n",
       "0           1               COPD  MOSF w/Sepsis  Yes    11142  11151.0   \n",
       "1           2      MOSF w/Sepsis  MOSF w/Sepsis   No    11799  11844.0   \n",
       "2           3  MOSF w/Malignancy  MOSF w/Sepsis  Yes    12083  12143.0   \n",
       "3           4                ARF  MOSF w/Sepsis   No    11146  11183.0   \n",
       "4           5      MOSF w/Sepsis  MOSF w/Sepsis   No    12035  12037.0   \n",
       "\n",
       "    dthdte  lstctdte death  cardiohx  ...  meta  hema  seps  trauma  ortho  \\\n",
       "0  11831.5     11382    No         0  ...    No    No    No      No     No   \n",
       "1  11844.0     11844   Yes         1  ...    No    No   Yes      No     No   \n",
       "2  11831.5     12400    No         0  ...    No    No    No      No     No   \n",
       "3  11183.0     11182   Yes         0  ...    No    No    No      No     No   \n",
       "4  12037.0     12036   Yes         0  ...    No    No    No      No     No   \n",
       "\n",
       "   adld3p   urin1   race      income  ptid  \n",
       "0     0.0  1927.0  white  Under $11k     5  \n",
       "1     0.0  1437.0  white  Under $11k     7  \n",
       "2     0.0   599.0  white    $25-$50k     9  \n",
       "3     0.0  1927.0  white    $11-$25k    10  \n",
       "4     0.0    64.0  white  Under $11k    11  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First approach\n",
    "def clean_data(data):\n",
    "    \"\"\"Basic data cleaning and handling missing values.\"\"\"\n",
    "    # Handle missing values\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == 'object' or data[column].dtype == 'category':\n",
    "            # For categorical columns, fill with the most frequent value\n",
    "            mode_value = data[column].mode()[0]\n",
    "            data[column] = data[column].fillna(mode_value)\n",
    "        else:\n",
    "            # For numerical columns, fill with the median\n",
    "            data[column] = data[column].fillna(data[column].median())\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "cleaned_data = clean_data(load_rhc_data()[0])\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n",
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n",
      "Training Mini TVAE model with the following parameters:\n",
      "  embedding_dim: 128\n",
      "  compress_dims: (128, 128)\n",
      "  decompress_dims: (128, 128)\n",
      "  l2scale: 1e-05\n",
      "  batch_size: 500\n",
      "  epochs: 500\n",
      "  loss_factor: 2\n",
      "  cuda: True\n",
      "  verbose: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "Loss: -81.235: 100%|██████████| 500/500 [10:14<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mini_tvae.MiniTVAE at 0x7ce3ff44beb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_tvae_model(data, discrete_columns, hyperparams=None):\n",
    "    \"\"\"\n",
    "    Create and train a MiniTVAE model with the provided hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : DataFrame\n",
    "        The training data\n",
    "    discrete_columns : list\n",
    "        List of discrete/categorical columns\n",
    "    hyperparams : dict, optional\n",
    "        Dictionary of hyperparameters for the MiniTVAE model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    MiniTVAE\n",
    "        The trained model\n",
    "    \"\"\"\n",
    "    # Default hyperparameters\n",
    "    default_params = {\n",
    "        'embedding_dim': 128,\n",
    "        'compress_dims': (128, 128),\n",
    "        'decompress_dims': (128, 128),\n",
    "        'l2scale': 1e-5,\n",
    "        'batch_size': 500,\n",
    "        'epochs': 500,\n",
    "        'loss_factor': 2,\n",
    "        'cuda': True,\n",
    "        'verbose': True\n",
    "    }\n",
    "    \n",
    "    # Use provided hyperparameters or default values\n",
    "    params = default_params.copy()\n",
    "    if hyperparams:\n",
    "        params.update(hyperparams)\n",
    "    \n",
    "    print(\"Training Mini TVAE model with the following parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = MiniTVAE(**params)\n",
    "    model.fit(data, discrete_columns)\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_tvae_model(clean_data(load_rhc_data()[0]), load_rhc_data()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n",
      "No discrete columns found in metadata. Inferring from data types...\n",
      "Loaded RHC dataset with 5735 rows and 63 columns\n",
      "Identified 21 discrete columns\n",
      "Training Mini TVAE model with the following parameters:\n",
      "  embedding_dim: 128\n",
      "  compress_dims: (128, 128)\n",
      "  decompress_dims: (128, 128)\n",
      "  l2scale: 1e-05\n",
      "  batch_size: 500\n",
      "  epochs: 500\n",
      "  loss_factor: 2\n",
      "  cuda: True\n",
      "  verbose: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/mfacotti/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss curves saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 62\u001b[0m plot_loss_over_epochs(\u001b[43mtrain_tvae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_rhc_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_rhc_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlosses_)\n",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m, in \u001b[0;36mtrain_tvae_model\u001b[0;34m(data, discrete_columns, hyperparams)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m MiniTVAE(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/martin/tVAE_project/test_mini_tvae/mini_tvae.py:138\u001b[0m, in \u001b[0;36mMiniTVAE.fit\u001b[0;34m(self, train_data, discrete_columns)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmini_data_trans\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataTransformer\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m DataTransformer()\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     train_data_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mtransform(train_data)\n",
      "File \u001b[0;32m~/martin/tVAE_project/test_mini_tvae/mini_data_trans.py:312\u001b[0m, in \u001b[0;36mDataTransformer.fit\u001b[0;34m(self, raw_data, discrete_columns)\u001b[0m\n\u001b[1;32m    310\u001b[0m     column_transform_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_discrete(raw_data[[column_name]])\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     column_transform_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_continuous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_info_list\u001b[38;5;241m.\u001b[39mappend(column_transform_info\u001b[38;5;241m.\u001b[39moutput_info)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dimensions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m column_transform_info\u001b[38;5;241m.\u001b[39moutput_dimensions\n",
      "File \u001b[0;32m~/martin/tVAE_project/test_mini_tvae/mini_data_trans.py:243\u001b[0m, in \u001b[0;36mDataTransformer._fit_continuous\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    238\u001b[0m column_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    239\u001b[0m gm \u001b[38;5;241m=\u001b[39m ClusterBasedNormalizer(\n\u001b[1;32m    240\u001b[0m     max_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_clusters),\n\u001b[1;32m    241\u001b[0m     weight_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weight_threshold,\n\u001b[1;32m    242\u001b[0m )\n\u001b[0;32m--> 243\u001b[0m \u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m num_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(gm\u001b[38;5;241m.\u001b[39mvalid_component_indicator)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ColumnTransformInfo(\n\u001b[1;32m    247\u001b[0m     column_name\u001b[38;5;241m=\u001b[39mcolumn_name,\n\u001b[1;32m    248\u001b[0m     column_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     output_dimensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m num_components,\n\u001b[1;32m    252\u001b[0m )\n",
      "File \u001b[0;32m~/martin/tVAE_project/test_mini_tvae/mini_data_trans.py:52\u001b[0m, in \u001b[0;36mClusterBasedNormalizer.fit\u001b[0;34m(self, data, column_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m BayesianGaussianMixture(\n\u001b[1;32m     45\u001b[0m         n_components\u001b[38;5;241m=\u001b[39meffective_max_clusters,\n\u001b[1;32m     46\u001b[0m         weight_concentration_prior_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_process\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Identify components with weights above threshold\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_component_indicator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_threshold)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:180\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/mixture/_base.py:247\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    244\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m    246\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[1;32m    250\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/mixture/_bayesian_mixture.py:739\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._m_step\u001b[0;34m(self, X, log_resp)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_weights(nk)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_means(nk, xk)\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_precisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/mixture/_bayesian_mixture.py:586\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._estimate_precisions\u001b[0;34m(self, nk, xk, sk)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the precisions parameters of the precision distribution.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m    'spherical' : (n_components,)\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    579\u001b[0m {\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_wishart_full,\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_wishart_tied,\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_wishart_diag,\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspherical\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_wishart_spherical,\n\u001b[1;32m    584\u001b[0m }[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type](nk, xk, sk)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_precision_cholesky\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariances_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:331\u001b[0m, in \u001b[0;36m_compute_precision_cholesky\u001b[0;34m(covariances, covariance_type)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m linalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(estimate_precision_error_message)\n\u001b[0;32m--> 331\u001b[0m         precisions_chol[k] \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_triangular\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcov_chol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m covariance_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    335\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m covariances\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/linalg/_basic.py:481\u001b[0m, in \u001b[0;36msolve_triangular\u001b[0;34m(a, b, trans, lower, unit_diagonal, overwrite_b, check_finite)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mSolve the equation ``a x = b`` for `x`, assuming a is a triangular matrix.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m \n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m a1 \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[0;32m--> 481\u001b[0m b1 \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_validated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected square matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/_lib/_util.py:539\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    537\u001b[0m a \u001b[38;5;241m=\u001b[39m toarray(a)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_ok:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject arrays are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_inexact:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_loss_over_epochs(loss_values, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the loss function across epochs, including Loss_1, Loss_2, and Global Loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    loss_values : DataFrame\n",
    "        DataFrame containing 'Epoch', 'Loss_1', 'Loss_2', and 'Loss_Global' columns.\n",
    "    save_path : str, optional\n",
    "        Path to save the image. If None, the image is not saved.\n",
    "    \"\"\"\n",
    "    # Group by epoch and calculate mean loss per epoch\n",
    "    epoch_loss = loss_values.groupby('Epoch').agg({\n",
    "        'Loss_1': 'mean',\n",
    "        'Loss_2': 'mean',\n",
    "        'Loss_Global': 'mean',\n",
    "    }).reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot mean Loss_1, Loss_2, and Global Loss per epoch\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epoch_loss['Epoch'], epoch_loss['Loss_1'], 'b-', linewidth=2, label='Reconstruction Loss (Loss_1)')\n",
    "    plt.plot(epoch_loss['Epoch'], epoch_loss['Loss_2'], 'r-', linewidth=2, label='KLD (Loss_2)')\n",
    "    plt.plot(epoch_loss['Epoch'], epoch_loss['Loss_Global'], 'g-', linewidth=2, label='Global Loss (Loss_1 + Loss_2)')\n",
    "    plt.title('Mean Loss Components per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot all batch losses across epochs (Loss_1)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for epoch in sorted(loss_values['Epoch'].unique()):\n",
    "        epoch_data = loss_values[loss_values['Epoch'] == epoch]\n",
    "        plt.scatter([epoch] * len(epoch_data), epoch_data['Loss_1'], \n",
    "                   alpha=0.5, s=10, color='blue', label='Loss_1' if epoch == 0 else \"\")\n",
    "    plt.title('Batch Reconstruction Loss (Loss_1)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot all batch losses across epochs (Loss_2)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for epoch in sorted(loss_values['Epoch'].unique()):\n",
    "        epoch_data = loss_values[loss_values['Epoch'] == epoch]\n",
    "        plt.scatter([epoch] * len(epoch_data), epoch_data['Loss_2'], \n",
    "                   alpha=0.5, s=10, color='red', label='Loss_2' if epoch == 0 else \"\")\n",
    "    plt.title('Batch KLD (Loss_2)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Loss curves saved to '{save_path}'\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_over_epochs(train_tvae_model(clean_data(load_rhc_data()[0]), load_rhc_data()[1]).losses_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(model, num_samples, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic data from the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : MiniTVAE\n",
    "        The trained model\n",
    "    num_samples : int\n",
    "        Number of samples to generate\n",
    "    save_path : str, optional\n",
    "        Path to save the synthetic data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        The generated synthetic data\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} synthetic samples...\")\n",
    "    synthetic_data = model.sample(num_samples)\n",
    "    \n",
    "    if save_path:\n",
    "        synthetic_data.to_csv(save_path, index=False)\n",
    "        print(f\"Synthetic data saved to '{save_path}'\")\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "# Complete usage example\n",
    "# def run_full_example():\n",
    "#     # Load data\n",
    "#     data, discrete_columns = load_rhc_data()\n",
    "#     if data is None:\n",
    "#         return\n",
    "#     \n",
    "#     # Display information about the data\n",
    "#     print(\"\\nData sample:\")\n",
    "#     print(data.head())\n",
    "#     print(\"\\nData types:\")\n",
    "#     print(data.dtypes)\n",
    "#     \n",
    "#     # Clean the data\n",
    "#     data = clean_data(data)\n",
    "#     \n",
    "#     # Custom hyperparameters (example)\n",
    "#     custom_params = {\n",
    "#         'embedding_dim': 128,\n",
    "#         'batch_size': 500,\n",
    "#         'epochs': 500,\n",
    "#         'cuda': True\n",
    "#     }\n",
    "#     \n",
    "#     # Train the model\n",
    "#     model = train_tvae_model(data, discrete_columns, custom_params)\n",
    "#     \n",
    "#     # Generate synthetic data\n",
    "#     synthetic_data = generate_synthetic_data(model, len(data), 'synthetic_rhc.csv')\n",
    "#     \n",
    "#     # Compare statistics\n",
    "#     compare_statistics(data, synthetic_data, discrete_columns)\n",
    "#     \n",
    "#     # Visualize losses\n",
    "#     plot_loss_over_epochs(model.loss_values, 'tvae_loss_curves.png')\n",
    "#     \n",
    "#     return model, data, synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_statistics(original_data, synthetic_data, discrete_columns, \n",
    "                       num_numeric=5, num_categorical=3, num_categories=5):\n",
    "    \"\"\"\n",
    "    Compare statistics between original and synthetic data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_data : DataFrame\n",
    "        The original data\n",
    "    synthetic_data : DataFrame\n",
    "        The synthetic data\n",
    "    discrete_columns : list\n",
    "        List of discrete/categorical columns\n",
    "    num_numeric : int, optional\n",
    "        Number of numeric columns to compare\n",
    "    num_categorical : int, optional\n",
    "        Number of categorical columns to compare\n",
    "    num_categories : int, optional\n",
    "        Number of categories to display per categorical column\n",
    "    \"\"\"\n",
    "    print(\"Comparing statistics between original and synthetic data:\")\n",
    "    \n",
    "    # Compare numeric columns\n",
    "    numeric_columns = original_data.select_dtypes(include=['number']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        print(\"\\nNumeric columns comparison:\")\n",
    "        for col in numeric_columns[:num_numeric]:\n",
    "            orig_mean = original_data[col].mean()\n",
    "            orig_std = original_data[col].std()\n",
    "            syn_mean = synthetic_data[col].mean()\n",
    "            syn_std = synthetic_data[col].std()\n",
    "            \n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(f\"  Original - Mean: {orig_mean:.4f}, Std: {orig_std:.4f}\")\n",
    "            print(f\"  Synthetic - Mean: {syn_mean:.4f}, Std: {syn_std:.4f}\")\n",
    "            print(f\"  Difference - Mean: {abs(orig_mean-syn_mean):.4f}, Std: {abs(orig_std-syn_std):.4f}\")\n",
    "    \n",
    "    # Compare categorical columns\n",
    "    if len(discrete_columns) > 0:\n",
    "        print(\"\\nCategorical columns comparison (value counts percentage):\")\n",
    "        for col in discrete_columns[:num_categorical]:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            orig_counts = original_data[col].value_counts(normalize=True).sort_index()\n",
    "            syn_counts = synthetic_data[col].value_counts(normalize=True).sort_index()\n",
    "            \n",
    "            # Combine indices to ensure we show all categories\n",
    "            all_cats = sorted(list(set(list(orig_counts.index) + list(syn_counts.index))))\n",
    "            \n",
    "            for cat in all_cats[:num_categories]:\n",
    "                orig_pct = orig_counts.get(cat, 0) * 100\n",
    "                syn_pct = syn_counts.get(cat, 0) * 100\n",
    "                print(f\"  {cat}: Original {orig_pct:.1f}%, Synthetic {syn_pct:.1f}%, Diff {abs(orig_pct-syn_pct):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
